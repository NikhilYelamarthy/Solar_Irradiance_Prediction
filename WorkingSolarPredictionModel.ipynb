{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error over the test set= 0.001 3.573168115704329\n",
      "variance of error over the test set 0.001 2.650197879688187\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "from IPython.display import display\n",
    "import scipy.optimize as opt\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir(\"D:\\python\")\n",
    "#Reading the input data into a data frame\n",
    "df = pd.read_csv(\"SolarPrediction.csv\")\n",
    "\n",
    "#Getting some useful paramters such as hour,month,year length of day etc. from the given input data and deleting unnecessary columns\n",
    "df['Time_conv'] =  pd.to_datetime(df['Time'], format='%H:%M:%S')\n",
    "df['hour'] = pd.to_datetime(df['Time_conv'], format='%H:%M:%S').dt.hour\n",
    "df['month'] = pd.to_datetime(df['UNIXTime'].astype(int), unit='s').dt.month\n",
    "df['year'] = pd.to_datetime(df['UNIXTime'].astype(int), unit='s').dt.year\n",
    "df['total_time'] = pd.to_datetime(df['TimeSunSet'], format='%H:%M:%S').dt.hour - pd.to_datetime(df['TimeSunRise'], format='%H:%M:%S').dt.hour\n",
    "\n",
    "A= df.to_numpy(dtype=None, copy=False)  \n",
    "X=A[:,[4,5,6,7,8,12,13,14,15]]\n",
    "\n",
    "#Normalising the input data\n",
    "for i in range(9):\n",
    " X[:,i]-=np.mean(X[:,i])\n",
    " X[:,i]/=np.std(X[:,i])\n",
    "y=A[:,4].flatten()\n",
    "\n",
    "#Defining the sigmoid and RELU activation functions and their derivatives(RELU was finally preferred due to better results)\n",
    "def sigmoiddef(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "def dersigmoiddef(x):\n",
    " return sigmoiddef(x)-sigmoiddef(x)*sigmoiddef(x)\n",
    "\n",
    "def reludef(x):\n",
    "  return max(0.0, x)\n",
    "def reluderdef(x):\n",
    "  if x>0:\n",
    "    return(1)\n",
    "  else:\n",
    "    return(0)\n",
    "\n",
    "sigmoid =np.frompyfunc(sigmoiddef,1,1)\n",
    "sigmoidder=np.frompyfunc(dersigmoiddef,1,1)\n",
    "relu =np.frompyfunc(reludef,1,1)\n",
    "reluder =np.frompyfunc(reluderdef,1,1)\n",
    "\n",
    "\n",
    "costs=[]\n",
    "#Defining a function which calculates the cost function of our neural network for a given training set,given set of weights(theta) and regularisation parameter-lamda\n",
    "def compute_cost(theta,X,y,lamda): \n",
    " z2=np.zeros((5,1))\n",
    " a1=np.zeros((9,1))\n",
    " a2=np.zeros((5,1))\n",
    " a3=np.zeros((1,1))\n",
    "    \n",
    " cost=0\n",
    "\n",
    " theta2,b2,theta3,b3=np.split(theta,[45,50,55],axis=0)\n",
    " theta2=np.resize(theta2,(5,9))\n",
    " b2=np.resize(b2,(5,1))\n",
    " theta3=np.resize(theta3,(1,5))\n",
    " b3=np.resize(b3,(1,1))\n",
    "    \n",
    " for i in range(10000):\n",
    "  a1=np.resize((X[i,:]).copy(),(9,1))\n",
    "  z2=(np.dot(theta2,a1))+b2\n",
    "  a2=relu(z2)\n",
    "  a3=theta3.dot(a2)+b3\n",
    "  cost+=(1/20000)*(a3[0]-y[i])*(a3[0]-y[i]) \n",
    "\n",
    " cost=cost+lamda*np.sum(np.square(theta2))+np.sum(np.square(theta3))\n",
    " costs.append(cost)\n",
    " return cost\n",
    "\n",
    "#Defining a forward propogation function which finds the predicted output of the neural network for a given a set of (weights)theta\n",
    "def forwardprop(theta,X):\n",
    " theta2,b2,theta3,b3=np.split(theta,[45,50,55],axis=0)\n",
    " theta2=np.resize(theta2,(5,9))\n",
    " b2=np.resize(b2,(5,1))\n",
    " theta3=np.resize(theta3,(1,5))\n",
    " b3=np.resize(b3,(1,1))\n",
    " a1=np.resize((X),(9,1))\n",
    " z2=(np.dot(theta2,a1))+b2\n",
    " a2=relu(z2)\n",
    " a3=theta3.dot(a2)+b3\n",
    " return(a3[0])\n",
    "\n",
    "#Defining a function which calculates the gradient vector of our cost function for a given training set,given set of weights(theta) and regularisation parameter-lamda\n",
    "#I used a backpropogation algorithm to implement this.\n",
    "def compute_grad(theta, X, y,lamda):\n",
    " theta2,b2,theta3,b3=np.split(theta,[45,50,55],axis=0)\n",
    " theta2=np.resize(theta2,(5,9))\n",
    " b2=np.resize(b2,(5,1))\n",
    " theta3=np.resize(theta3,(1,5))\n",
    " b3=np.resize(b3,(1,1))\n",
    " theta2grad= np.zeros((5,9))\n",
    " theta3grad = np.zeros((1,5))\n",
    " b2grad = np.zeros((5,1))\n",
    " b3grad=np.zeros((1,1))\n",
    " \n",
    " for i in range(10000):\n",
    "  a1=np.resize((X[i,:]),(9,1))\n",
    "  z2=(np.dot(theta2,a1))+b2\n",
    "  a2=relu(z2)\n",
    "  a3=np.dot(theta3,a2)+b3\n",
    "  del3=((a3[0]-y[i])/10000)\n",
    "  del3=del3[0]\n",
    "  temp=np.multiply(theta3.T,reluder(z2))\n",
    "  del2=del3*temp\n",
    "  theta3grad = theta3grad + np.multiply(del3,a2.T) \n",
    "  b3grad = b3grad + del3\n",
    "  theta2grad = theta2grad + np.dot(del2,a1.T)\n",
    "  b2grad = b2grad + del2\n",
    "  grad=np.zeros(56)\n",
    " theta3grad =theta3grad+2*lamda*theta3\n",
    " theta2grad =theta2grad+2*lamda*theta2\n",
    " for s in range(45):\n",
    "            for j in range(5):\n",
    "                for k in range(9):\n",
    "                    grad[s]=theta2grad[j][k]\n",
    " for s in range(45,50):\n",
    "            for j in range(5):\n",
    "                grad[s]=b2grad[j][0]\n",
    "                \n",
    " for s in range(50,55):\n",
    "        for j in range(5):\n",
    "             grad[s]=theta3grad[0][j]\n",
    " grad[55]=b3grad[0][0]\n",
    "\n",
    " return grad\n",
    "\n",
    "#Creating an initial randomized theta vector \n",
    "INIT_EPSILON=2\n",
    "initialtheta=np.random.rand(56)* (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "initialtheta=np.reshape(initialtheta,(56,1))\n",
    "\n",
    "y=np.reshape(y,(len(y),1))\n",
    "y=np.squeeze(y)\n",
    "\n",
    "#Setting the regularisation parameter lamda as 0.001 (Upon doing a parametric sweep, obtained the best results for this value of lamda)\n",
    "lamda=0.001\n",
    "\n",
    "#Using a built-in optimise function from the scipy.optimize module and passing to it the cost function and gradient funtion we defined above.\n",
    "#The function takes in the initial theta, cost function and gradient function and minimises the cost function and returns the final set of weights: theta\n",
    "theta=initialtheta\n",
    "for epoch in range(30):\n",
    "        theta = (opt.fmin_tnc(func=compute_cost, x0=np.squeeze(theta), fprime=compute_grad, args=(X, y,lamda)))[0]\n",
    "        \n",
    "        \n",
    "error=[];res=[]\n",
    "\n",
    "#Finding the test set errors\n",
    "for k in range(10000,20000):\t\t\n",
    "       error.append(np.abs(forwardprop(theta,X[k,:])-y[k]))\n",
    "       res.append(np.abs(forwardprop(theta,X[k,:])))\n",
    "\n",
    "#Printing the mean error through the test set\n",
    "print('mean error over the test set=',lamda,sum(error)/len(error))\n",
    "print('variance of error over the test set',lamda,np.std(error))\n",
    "\n",
    "show()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
